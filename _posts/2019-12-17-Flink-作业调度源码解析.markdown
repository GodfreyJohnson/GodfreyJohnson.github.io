---
layout: post
title: Flink作业调度
date: 2019-12-18 10:00:24.000000000 +08:00
categories: [flink, scheduler, 作业调度]
---

Flink作业调度，总体而言包括以下几个部分：Task的切分、Task的分配和Task的运行。
这几个步骤包含了Flink Job调度的完整流程，接下来进行详细介绍。

# Task的切分

首先需要明白存在的切分类型。
- 纵向切分：用户的不同操作(map/reduce)，会对数据进行不同的处理，同时会被包装成不同的Operator，
每条数据都是从上到下流过整个DataFlow，因此会很自然的通过Operator进行纵向切分，
分成许多不同的Operator再分别进行处理，其中也通过Operator Chain的优化对整个DataFlow进行优化。
- 横向切分：分布式计算场景下，用户可以指定并行度来将数据分到不同节点进行处理，
提高大数据场景下处理的能力，同时在数据增加时，可以更加方便的进行资源的扩展，其实现是通过Slot完成资源的分配。
<img src="/assets/images/flinktaskschains.svg" />

## 纵向切分-Operator Chain

为什么要将多个Operator包装成一个Operator Chain呢？我们都知道Flink是对每个Operator都有一个单独的线程进行执行。
如上一个Task完成，需要切换到运行下一个Task的线程。Operator Chain的优化则可以带来以下好处：
- 减少线程之间的切换
- 数据在同一个线程中进行共享
- 提高吞吐量

如何判断是否需要进行Operator Chain整合呢？可以通过下图中了解其具体的标准
<img src="/assets/images/flinkoperatorchain.png" />

## 横向切分-Slot

同一个TaskManager上有多个Slot的好处：
- 共享TCP连接和心跳
- 共享数据和数据结构

Slot中有整个Pipeline的好处：
- slot数量即为最高并行度，不需要再计算究竟有多少Task
- 有效提高资源利用率，如多个operator在同一个线程中完成

## sharedSlotGroup

用户可以自由打开和关闭chaining，也可以指定从哪个Transformation开始进行Chaining操作，自由度很大。
- sharedSlotGroup： 用户指定相同sharedslotgroup的Task会在同一个slot当中
- colocationGroup：相同index的Task会在同一个slot中
- AlloctedSlot：由JM向RM申请Slot资源，RM将指定的TM分配给JM，此TM会向JM注册，此为AllocatedSlot
- Slot：资源调度的最小单元，是在AllocatedSlot基础上，再加上一些字段，如这个Slot执行什么内容，或者状态等
- SimpleSlot：只负责一个Task的运行
- SharedSlot：可以包含多个SimpleSlot，对应多个Task
- SlotSharingGroupAssignment：用于管理许多SharedSlots，其中的allSlots变量用于保存所有SharedSlot，
availableSlotsPerJid用于记录JobVertexID与ResourceID的对应关系

# Task的分配

上文介绍了Slot的相关概念，此节则是需要通过调度，为Task分配具体的Slot，核心函数为getSlotForTask。
调度逻辑是优先为JobVertex分配想要的Slot，没有想要的就随机分配Slot，可以local也可以是non-local。
其中Instance对应TaskManager。

```java
SimpleSlot getSlotForTask(JobVertexID vertexID, Iterable<Instance> locationPreferences) {
		synchronized (lock) {
			Pair<SharedSlot, Locality> p = getSlotForTaskInternal(vertexID, locationPreferences, false);

			if (p != null) {
				SharedSlot ss = p.getLeft();
				SimpleSlot slot = ss.allocateSubSlot(vertexID);
				slot.setLocality(p.getRight());
				return slot;
			} else {
				return null;
			}
		}
	}

private SimpleSlot addSharedSlotAndAllocateSubSlot(
            SharedSlot sharedSlot, Locality locality, JobVertexID groupId, CoLocationConstraint constraint) {

        final ResourceID location = sharedSlot.getTaskManagerID();

        synchronized (lock) {
            
            SimpleSlot subSlot;
            AbstractID groupIdForMap;
            
            // add to the total bookkeeping
            if (!allSlots.add(sharedSlot)) { //加到allSlots中
                throw new IllegalArgumentException("Slot was already contained in the assignment group");
            }
                    
            if (constraint == null) {
                // allocate us a sub slot to return
                subSlot = sharedSlot.allocateSubSlot(groupId); //简单的allocate一个simpleSlot
                groupIdForMap = groupId;
            }
            else { //如果有CoLocationConstraint
                
            }
            
            if (subSlot != null) {
                // preserve the locality information
                subSlot.setLocality(locality);
                
                // let the other groups know that this slot exists and that they
                // can place a task into this slot.
                boolean entryForNewJidExists = false;
                
                for (Map.Entry<AbstractID, Map<ResourceID, List<SharedSlot>>> entry :
                     availableSlotsPerJid.entrySet()) {
                    // there is already an entry for this groupID
                    if (entry.getKey().equals(groupIdForMap)) {
                        entryForNewJidExists = true;
                        continue;
                    }

                    Map<ResourceID, List<SharedSlot>> available = entry.getValue();
                    //对于其他的jobVertex，把sharedSlot加上去
                    putIntoMultiMap(available, location, sharedSlot); 
                }

                // make sure an empty entry exists for this group, if no other entry exists
                if (!entryForNewJidExists) { //如果存在参数中的groupId，那么就把它的slot信息清空
                    availableSlotsPerJid.put(groupIdForMap, 
                        new LinkedHashMap<ResourceID, List<SharedSlot>>());
                }

                return subSlot;
            }
        }
        // end synchronized (lock)
    }
```

## Slot分配具体过程

Flink调度包含两个重要的原则：
- 同一个Operator的各个Subtask不能处于同一个SharedSlot，如FlatMap[1]和FlatMap[2]是不能在同一个SharedSlot中的
- Flink分配Slot是按照拓扑排序从Source到Sink进行的。
例如WordCount（Source并行度为1，其他并行度为2），那么调度的顺序依次是：Source -> FlatMap[1] -> FlatMap[2] -> KeyAgg->Sink[1] -> KeyAgg->Sink[2]。假设现在有2个TaskManager，每个只有1个slot（为简化问题），那么分配slot的过程如图所示：
<img src="/assets/images/flinksharedslot.png" />

- 为Source分配slot。首先，我们从TaskManager1中分配出一个SharedSlot。并从SharedSlot中为Source分配出一个SimpleSlot。如上图中的①和②。
- 为FlatMap[1]分配slot。目前已经有一个SharedSlot，则从该SharedSlot中分配出一个SimpleSlot用来部署FlatMap[1]。如上图中的③。
- 为FlatMap[2]分配slot。由于TaskManager1的SharedSlot中已经有同operator的FlatMap[1]了，我们只能分配到其他SharedSlot中去。从TaskManager2中分配出一个SharedSlot，并从该SharedSlot中为FlatMap[2]分配出一个SimpleSlot。如上图的④和⑤。
- 为Key->Sink[1]分配slot。目前两个SharedSlot都符合条件，从TaskManager1的SharedSlot中分配出一个SimpleSlot用来部署Key->Sink[1]。如上图中的⑥。
- 为Key->Sink[2]分配slot。TaskManager1的SharedSlot中已经有同operator的Key->Sink[1]了，则只能选择另一个SharedSlot中分配出一个SimpleSlot用来部署Key->Sink[2]。如上图中的⑦。

## JobManager调度

JM中主要包含以下组件：
- BlobServer：用来管理二进制文件管理，如对用户上传的Jar会进行保存，TM可以从此下载用户Jar
- InstanceManager：用于管理目前存活的TM组件，记录TM心跳信息
- CompletedCheckpointStore：存储完成的Checkpoint信息
- MemoryArchivist：保存提交的Flink作业相关信息，如JobGraph

在启动JM后，会将Client传入的JobGraph转化成ExecutionGraph，ExecutionGraph会调用scheduleForExecution
，并最终对Execution调用deploy方法进行执行。

deploy方法会对每个ExecutionVertex分配Slot，并检查Slot所有TM状态，同时将当前Vertex状态变成deploying，
接下来生成了一个TaskDeploymentDescriptor，然后交给taskManagerGateway.submitTask() 方法执行。
后续的部分，就属于TaskManager的范畴了。



```java
/**
	 * Deploys the execution to the previously assigned resource.
	 *
	 * @throws JobException if the execution cannot be deployed to the assigned resource
	 */
	public void deploy() throws JobException {
		assertRunningInJobMasterMainThread();

		final LogicalSlot slot  = assignedResource;

		checkNotNull(slot, "In order to deploy the execution we first have to assign a resource via tryAssignResource.");

		// Check if the TaskManager died in the meantime
		// This only speeds up the response to TaskManagers failing concurrently to deployments.
		// The more general check is the rpcTimeout of the deployment call
		if (!slot.isAlive()) {
			throw new JobException("Target slot (TaskManager) for deployment is no longer alive.");
		}

		// make sure exactly one deployment call happens from the correct state
		// note: the transition from CREATED to DEPLOYING is for testing purposes only
		ExecutionState previous = this.state;
		if (previous == SCHEDULED || previous == CREATED) {
			if (!transitionState(previous, DEPLOYING)) {
				// race condition, someone else beat us to the deploying call.
				// this should actually not happen and indicates a race somewhere else
				throw new IllegalStateException("Cannot deploy task: Concurrent deployment call race.");
			}
		}
		else {
			// vertex may have been cancelled, or it was already scheduled
			throw new IllegalStateException("The vertex must be in CREATED or SCHEDULED state to be deployed. Found state " + previous);
		}

		if (this != slot.getPayload()) {
			throw new IllegalStateException(
				String.format("The execution %s has not been assigned to the assigned slot.", this));
		}

		try {

			// race double check, did we fail/cancel and do we need to release the slot?
			if (this.state != DEPLOYING) {
				slot.releaseSlot(new FlinkException("Actual state of execution " + this + " (" + state + ") does not match expected state DEPLOYING."));
				return;
			}

			if (LOG.isInfoEnabled()) {
				LOG.info(String.format("Deploying %s (attempt #%d) to %s", vertex.getTaskNameWithSubtaskIndex(),
						attemptNumber, getAssignedResourceLocation()));
			}

			final TaskDeploymentDescriptor deployment = TaskDeploymentDescriptorFactory
				.fromExecutionVertex(vertex, attemptNumber)
				.createDeploymentDescriptor(
					slot.getAllocationId(),
					slot.getPhysicalSlotNumber(),
					taskRestore,
					producedPartitions.values());

			// null taskRestore to let it be GC'ed
			taskRestore = null;

			final TaskManagerGateway taskManagerGateway = slot.getTaskManagerGateway();

			final ComponentMainThreadExecutor jobMasterMainThreadExecutor =
				vertex.getExecutionGraph().getJobMasterMainThreadExecutor();

			// We run the submission in the future executor so that the serialization of large TDDs does not block
			// the main thread and sync back to the main thread once submission is completed.
			CompletableFuture.supplyAsync(() -> taskManagerGateway.submitTask(deployment, rpcTimeout), executor)
				.thenCompose(Function.identity())
				.whenCompleteAsync(
					(ack, failure) -> {
						// only respond to the failure case
						if (failure != null) {
							if (failure instanceof TimeoutException) {
								String taskname = vertex.getTaskNameWithSubtaskIndex() + " (" + attemptId + ')';

								markFailed(new Exception(
									"Cannot deploy task " + taskname + " - TaskManager (" + getAssignedResourceLocation()
										+ ") not responding after a rpcTimeout of " + rpcTimeout, failure));
							} else {
								markFailed(failure);
							}
						}
					},
					jobMasterMainThreadExecutor);

		}
		catch (Throwable t) {
			markFailed(t);
			ExceptionUtils.rethrow(t);
		}
	}
```

# Task的运行

TaskManager是作业调度的基本单位，主要包含内存管理、IO管理和通信管理相关内容。

- MemoryManager：管理着用于排序、哈希和缓存等操作所需要的内存。内存管理以Segment为基本单位，默认32k
- IOManager：提供同步和异步两种写方式
- NetworkEnvironment：网络通信组件，对中间结果或者数据交换的内容，不管保留在本地，还是传输给别的TM，
都会经过NetworkBufferPool进行处理。

其次我们需要明确Task的真正的逻辑：
- 确认Slot资源
- 连接JM/BlobServer
- 反序列化算子
- 收集Task信息
- 创建新的Task

## Task的生成

在上文中的deploy方法中，创建了TaskDeploymentDescriptor，其中包含了执行时所需要的所有内容，如输入的InputGate和
输出的ResultPartition。

```java
// create the reader and writer structures

		final String taskNameWithSubtasksAndId =
				Task.getTaskNameWithSubtaskAndID(taskName, subtaskIndex, parallelism, executionId);

		List<ResultPartitionDeploymentDescriptor> partitions = tdd.getProducedPartitions();
		List<InputGateDeploymentDescriptor> consumedPartitions = tdd.getInputGates();

		// Produced intermediate result partitions
		this.producedPartitions = new ResultPartition[partitions.size()];
		this.writers = new ResultPartitionWriter[partitions.size()];

		for (int i = 0; i < this.producedPartitions.length; i++) {
			ResultPartitionDeploymentDescriptor desc = partitions.get(i);
			ResultPartitionID partitionId = new ResultPartitionID(desc.getPartitionId(), executionId);

			this.producedPartitions[i] = new ResultPartition(
					taskNameWithSubtasksAndId,
					jobId,
					partitionId,
					desc.getPartitionType(),
					desc.getNumberOfSubpartitions(),
					networkEnvironment.getPartitionManager(),
					networkEnvironment.getPartitionConsumableNotifier(),
					ioManager,
					networkEnvironment.getDefaultIOMode());

			this.writers[i] = new ResultPartitionWriter(this.producedPartitions[i]);
		}

		// Consumed intermediate result partitions
		this.inputGates = new SingleInputGate[consumedPartitions.size()];
		this.inputGatesById = new HashMap<IntermediateDataSetID, SingleInputGate>();

		for (int i = 0; i < this.inputGates.length; i++) {
			SingleInputGate gate = SingleInputGate.create(
					taskNameWithSubtasksAndId, jobId, executionId, consumedPartitions.get(i), networkEnvironment);

			this.inputGates[i] = gate;
			inputGatesById.put(gate.getConsumedResultId(), gate);
		}
```
最后生成一个可执行的线程，执行的内容为其自己，方便后续进行真正的执行。

## Task启动与状态管理


参考
- https://ci.apache.org/projects/flink/flink-docs-release-1.9/internals/job_scheduling.html
- https://clay4444.github.io/posts/149d881d/
- http://wuchong.me/blog/2016/05/09/flink-internals-understanding-execution-resources/
- Slot类型：https://www.cnblogs.com/fxjwind/p/6703312.html
- Slot分配：https://zhuanlan.zhihu.com/p/36525639

若喜爱本文章，可关注公众号，方便获取更新信息。
<img src="/assets/images/qrcode.jpg" />