---
layout: post
title: 神经网络的学习
date: 2019-11-17 20:00:24.000000000 +08:00
categories: [deep learning, ML, 神经网络]
---

原文：深度学习入门-第4章-神经网络的学习

源码：https://github.com/oreilly-japan/deep-learning-from-scratch

# 从数据中学习

- 难点：如何让机器和人一样自动识别5这个手写的数字？
- 方案：
    - 机器学习：先撮特征量（SIFT/SURF/HOG），再将转换后的向量，通过SVM/KNN等分类器进行机器学习。 
    - 深度学习：直接从原始数据中获得目标结果输出。优点是，所有的问题可以用相同的流程进行解决。

## 错误个数
如对预测错误个数或者比例进行统计，是粗粒度的损失函数，不能具体体现相同错误率的模型中，哪个会有更好的表现。

## 均方差
采用梯度下降的方式，输出的曲线是波动的，可能只能找到局部最优，不能找到全局最优，还有可能学习速率特别慢，所以一般也不会采用。

## 交叉熵误差
- 公式：对预测值取对数，乘以正确值，再对向量中的每个求和，再取负。
![crossentropyerror](./img/crossentropyerror.svg)
- 优点：本身是凸函数，所以有很好的收敛性，不像均方差只获取局部最优。
![crossgraph](./img/crossgraph.jpg)

## mini batch
当数据量太大的时候，不可能将所有数据都进行误差的统计，这样会非常耗时，因此会采用抽样的方式对模型进行验证。

```
np.random.choice(60000, 10)
```

随机从0-59999取10个索引，再对其进行交叉熵误差的处理。

## 梯度
由全部变量的偏导数汇总而成的向量成为梯度。

在机器学习和深度学习中，都是为寻找最优参数，得到的最优参数可使损失函数的值最小。但损失函数可能很复杂，因此通过梯度的方式不断寻找损失函数最小值。

学习率(一塔)需要预先设定，梯度下降时，可以按照这个比率进行下降。但学习率的设置需要有较好的控制：
- 学习率大，容易振荡，损失值容易爆炸，适合在刚开始训练的时候，如设置0.01-0.001
- 学习率小，容易过拟合，手链速度慢，适合在一定轮数后，逐渐减小