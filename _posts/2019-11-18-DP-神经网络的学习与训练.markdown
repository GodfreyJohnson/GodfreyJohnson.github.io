---
layout: post
title: 神经网络的学习
date: 2019-11-17 20:00:24.000000000 +08:00
categories: [deep learning, ML, 神经网络]
---

原文：深度学习入门-第4章-神经网络的学习

源码：https://github.com/oreilly-japan/deep-learning-from-scratch

# 从数据中学习

- 难点：如何让机器和人一样自动识别5这个手写的数字？
- 方案：
    - 机器学习：先撮特征量（SIFT/SURF/HOG），再将转换后的向量，通过SVM/KNN等分类器进行机器学习。 
    - 深度学习：直接从原始数据中获得目标结果输出。优点是，所有的问题可以用相同的流程进行解决。

## 训练数据和测试数据

机器学习中，将数据分为训练数据和测试数据，是为了评价模型的**泛化性**。介绍下机器学习中基本概念：

- 监督学习(supervised learning)：如分类(classification)，训练是为了寻找特征与标签的关系，在新的数据到来时，即使没有标签，也可以推导出其标签；
- 无监督学习(unsupervised learning)：如聚类(clustering)，没有标签与标准答案，需要通过训练来发现其内在联系；
- 过拟合(overfitting)：通过增加模型复杂度，对训练数据有非常好的支持，但遇到新数据，模型表现非常差的情况；
- 欠拟合：模型未能找到训练数据间的内在关系，所以即使用训练数据进行测试，表现也特别差。

## 损失函数

用于反映神经网络性能的指标，通常为均方误差计算方式等进行反映。

- Error to Bias：代表着算法的拟合能力，具体为预测值与真实值的距离的期望，通过建立多个模型来估计这个误差期望值。
- Error to Variance：代表算法的鲁棒性，具体为预测值与真实值的方差。

![biasandvar](img/biasandvariance.jpg)

举例而言，机器学习的过程类似于打靶。Bias反映的是朝着靶心瞄得准不准，靶心为真实值，如果靶心都瞄得不准，效果肯定很差，表现为欠拟合现象。Variance反映的是子弹瞄准打出去后，效果怎么样，如果每一次预测时离靶心都偏离较大，说明模型只对训练数据效果好，对新数据表现很差，表现为过拟合现象。

## 错误个数
如对预测错误个数或者比例进行统计，是粗粒度的损失函数，不能具体体现相同错误率的模型中，哪个会有更好的表现。

## 均方差
采用梯度下降的方式，输出的曲线是波动的，可能只能找到局部最优，不能找到全局最优，还有可能学习速率特别慢，所以一般也不会采用。

## 交叉熵误差
- 公式：对预测值取对数，乘以正确值，再对向量中的每个求和，再取负。
![crossentropyerror](img/crossentropyerror.svg)
- 优点：本身是凸函数，所以有很好的收敛性，不像均方差只获取局部最优。
![crossgraph](img/crossgraph.jpg)

## mini batch
当数据量太大的时候，不可能将所有数据都进行误差的统计，这样会非常耗时，因此会采用抽样的方式对模型进行验证。

```
np.random.choice(60000, 10)
```

随机从0-59999取10个索引，再对其进行交叉熵误差的处理。

## 梯度
由全部变量的偏导数汇总而成的向量成为梯度。

在机器学习和深度学习中，都是为寻找最优参数，得到的最优参数可使损失函数的值最小。但损失函数可能很复杂，因此通过梯度的方式不断寻找损失函数最小值。

学习率需要预先设定，梯度下降时，可以按照这个比率进行下降。但学习率的设置需要有较好的控制：
- 学习率大，容易振荡，损失值容易爆炸，适合在刚开始训练的时候，如设置0.01-0.001
- 学习率小，容易过拟合，手链速度慢，适合在一定轮数后，逐渐减小